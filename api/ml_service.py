"""
Client helpers for invoking the external machine-learning inference service.
"""

from __future__ import annotations

import json
import logging
import os
import random
from typing import Any, Dict, List, Optional

import requests
from huggingface_hub import InferenceClient

logger = logging.getLogger(__name__)

_STUB_MEALS = [
  {
    "food": "Grilled Chicken Salad",
    "calories": 420,
    "ingredients": [
      "Grilled Chicken Breast",
      "Mixed Greens",
      "Cherry Tomatoes",
      "Cucumber",
      "Balsamic Vinaigrette",
    ],
    "macros": {"protein": 38, "carbohydrates": 18, "fats": 18},
  },
  {
    "food": "Veggie Power Bowl",
    "calories": 510,
    "ingredients": [
      "Quinoa",
      "Roasted Sweet Potato",
      "Black Beans",
      "Avocado",
      "Lime Dressing",
    ],
    "macros": {"protein": 21, "carbohydrates": 62, "fats": 20},
  },
  {
    "food": "Salmon Sushi Roll",
    "calories": 360,
    "ingredients": [
      "Sushi Rice",
      "Fresh Salmon",
      "Nori",
      "Cucumber",
      "Soy Sauce",
    ],
    "macros": {"protein": 24, "carbohydrates": 44, "fats": 10},
  },
  {
    "food": "Mediterranean Wrap",
    "calories": 480,
    "ingredients": [
      "Whole Wheat Tortilla",
      "Hummus",
      "Feta Cheese",
      "Kalamata Olives",
      "Spinach",
    ],
    "macros": {"protein": 19, "carbohydrates": 48, "fats": 22},
  },
  {
    "food": "Spaghetti Bolognese",
    "calories": 610,
    "ingredients": [
      "Spaghetti",
      "Tomato Sauce",
      "Ground Beef",
      "Parmesan Cheese",
      "Basil",
    ],
    "macros": {"protein": 32, "carbohydrates": 68, "fats": 22},
  },
]


def _build_stub_prediction() -> Dict[str, Any]:
  """
  Generate a synthetic prediction payload when the real ML service is unavailable.

  The goal is to mimic the production response structure closely enough for
  end-to-end testing without depending on the external inference stack.
  """
  template = random.choice(_STUB_MEALS)
  jitter = random.randint(-40, 40)
  calories = max(120, template["calories"] + jitter)

  scale = calories / template["calories"]
  nutrition = {
    "calories": calories,
    "proteins": max(1, int(round(template["macros"]["protein"] * scale))),
    "carbohydrates": max(1, int(round(template["macros"]["carbohydrates"] * scale))),
    "fats": max(1, int(round(template["macros"]["fats"] * scale))),
  }

  return {
    "food": template["food"],
    "calories": calories,
    "ingredients": template["ingredients"],
    "nutrition_facts": nutrition,
    "confidence": round(random.uniform(0.74, 0.93), 2),
    "notes": "Generated by ML stub",
  }


class MLServiceError(RuntimeError):
  """Raised when the external ML service returns an error."""


def call_ml_service(
  image_bytes: bytes,
  metadata: Dict[str, Any],
  *,
  url: str,
  api_key: Optional[str] = None,
  timeout: int = 30,
) -> Dict[str, Any]:
  """
  Send the uploaded image (and extracted metadata) to the remote ML API.

  Parameters
  ----------
  image_bytes:
      Raw image contents supplied by the client.
  metadata:
      EXIF-derived metadata that may enhance the ML model's accuracy.
  url:
      Fully-qualified endpoint for the ML service prediction REST API.
  api_key:
      Optional bearer token injected as ``Authorization`` header.
  timeout:
      Request timeout in seconds.
  """
  if not url:
    raise MLServiceError("ML service URL is not configured.")

  stub_mode = os.environ.get("ML_SERVICE_MODE", "").strip().lower()
  normalised_url = url.strip().lower()
  if stub_mode == "stub" or normalised_url in {"stub", "mock"} or normalised_url.startswith("stub://"):
    logger.info("Returning synthetic prediction from ML stub (mode=%s, url=%s).", stub_mode or "unset", url)
    return _build_stub_prediction()

  files = {
    "image": ("upload.jpg", image_bytes, "application/octet-stream"),
  }
  data = {
    "metadata": json.dumps(metadata or {}),
  }
  headers = {}
  if api_key:
    headers["Authorization"] = f"Bearer {api_key}"

  try:
    response = requests.post(url, files=files, data=data, headers=headers, timeout=timeout)
  except requests.RequestException as exc:  # pragma: no cover
    raise MLServiceError(f"ML service request failed: {exc}") from exc

  if not response.ok:
    raise MLServiceError(
      f"ML service responded with {response.status_code}: {response.text}"
    )

  try:
    return response.json()
  except ValueError as exc:  # pragma: no cover
    raise MLServiceError("ML service did not return JSON.") from exc


def _serialise_hf_predictions(predictions: Any, top_k: int = 5) -> List[Dict[str, Any]]:
  """Convert Hugging Face classification outputs into JSON-friendly dicts."""
  serialised: List[Dict[str, Any]] = []
  for candidate in predictions or []:
    label = None
    score = None
    if isinstance(candidate, dict):
      label = candidate.get("label") or candidate.get("class")
      score = candidate.get("score") or candidate.get("confidence")
    else:
      label = getattr(candidate, "label", None)
      score = getattr(candidate, "score", None)

    if label is None:
      continue
    try:
      score_value = float(score) if score is not None else 0.0
    except (TypeError, ValueError):
      score_value = 0.0

    serialised.append(
      {
        "label": str(label),
        "score": score_value,
      }
    )
    if top_k and len(serialised) >= top_k:
      break
  return serialised


def classify_food_with_hf(
  image_bytes: bytes,
  *,
  api_key: str,
  model: str = "nateraw/food",
  provider: Optional[str] = "auto",
  top_k: int = 5,
) -> Dict[str, Any]:
  """
  Run the uploaded image through a Hugging Face Inference Provider model.

  Parameters
  ----------
  image_bytes:
      Raw uploaded bytes.
  api_key:
      HF token with access to Inference Endpoints / Providers.
  model:
      Repository ID of the image classification model (default: ``nateraw/food``).
  provider:
      Optional provider hint (``auto`` lets HF choose automatically).
  top_k:
      Number of ranked candidates to include in the response payload.
  """
  if not api_key:
    raise MLServiceError("HF token is not configured.")
  cleaned_model = (model or "").strip()
  if not cleaned_model:
    raise MLServiceError("HF model ID is not configured.")

  client_kwargs: Dict[str, Any] = {"api_key": api_key}
  provider_hint = (provider or "").strip()
  if provider_hint:
    client_kwargs["provider"] = provider_hint
  client = InferenceClient(**client_kwargs)

  try:
    predictions = client.image_classification(image=image_bytes, model=cleaned_model)
  except Exception as exc:  # pragma: no cover - network/SDK errors
    raise MLServiceError(f"Hugging Face inference failed: {exc}") from exc

  serialised = _serialise_hf_predictions(predictions, top_k=top_k)
  if not serialised:
    raise MLServiceError("Hugging Face inference returned no predictions.")

  top_prediction = serialised[0]
  confidence = top_prediction.get("score") or 0.0
  return {
    "food": top_prediction.get("label") or "Meal",
    "calories": 0,
    "ingredients": [],
    "nutrition_facts": {"calories": 0},
    "confidence": round(confidence, 4),
    "hf_predictions": serialised,
    "notes": f"Hugging Face {cleaned_model}",
  }


__all__ = ["call_ml_service", "classify_food_with_hf", "MLServiceError"]
