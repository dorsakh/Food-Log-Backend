"""
Client helpers for invoking the external machine-learning inference service.
"""

from __future__ import annotations

import json
import logging
import os
import random
from typing import Any, Dict, Optional

import requests

logger = logging.getLogger(__name__)

_STUB_MEALS = [
  {
    "food": "Grilled Chicken Salad",
    "calories": 420,
    "ingredients": [
      "Grilled Chicken Breast",
      "Mixed Greens",
      "Cherry Tomatoes",
      "Cucumber",
      "Balsamic Vinaigrette",
    ],
    "macros": {"protein": 38, "carbohydrates": 18, "fats": 18},
  },
  {
    "food": "Veggie Power Bowl",
    "calories": 510,
    "ingredients": [
      "Quinoa",
      "Roasted Sweet Potato",
      "Black Beans",
      "Avocado",
      "Lime Dressing",
    ],
    "macros": {"protein": 21, "carbohydrates": 62, "fats": 20},
  },
  {
    "food": "Salmon Sushi Roll",
    "calories": 360,
    "ingredients": [
      "Sushi Rice",
      "Fresh Salmon",
      "Nori",
      "Cucumber",
      "Soy Sauce",
    ],
    "macros": {"protein": 24, "carbohydrates": 44, "fats": 10},
  },
  {
    "food": "Mediterranean Wrap",
    "calories": 480,
    "ingredients": [
      "Whole Wheat Tortilla",
      "Hummus",
      "Feta Cheese",
      "Kalamata Olives",
      "Spinach",
    ],
    "macros": {"protein": 19, "carbohydrates": 48, "fats": 22},
  },
  {
    "food": "Spaghetti Bolognese",
    "calories": 610,
    "ingredients": [
      "Spaghetti",
      "Tomato Sauce",
      "Ground Beef",
      "Parmesan Cheese",
      "Basil",
    ],
    "macros": {"protein": 32, "carbohydrates": 68, "fats": 22},
  },
]


def _build_stub_prediction() -> Dict[str, Any]:
  """
  Generate a synthetic prediction payload when the real ML service is unavailable.

  The goal is to mimic the production response structure closely enough for
  end-to-end testing without depending on the external inference stack.
  """
  template = random.choice(_STUB_MEALS)
  jitter = random.randint(-40, 40)
  calories = max(120, template["calories"] + jitter)

  scale = calories / template["calories"]
  nutrition = {
    "calories": calories,
    "proteins": max(1, int(round(template["macros"]["protein"] * scale))),
    "carbohydrates": max(1, int(round(template["macros"]["carbohydrates"] * scale))),
    "fats": max(1, int(round(template["macros"]["fats"] * scale))),
  }

  return {
    "food": template["food"],
    "calories": calories,
    "ingredients": template["ingredients"],
    "nutrition_facts": nutrition,
    "confidence": round(random.uniform(0.74, 0.93), 2),
    "notes": "Generated by ML stub",
  }


class MLServiceError(RuntimeError):
  """Raised when the external ML service returns an error."""


def call_ml_service(
  image_bytes: bytes,
  metadata: Dict[str, Any],
  *,
  url: str,
  api_key: Optional[str] = None,
  timeout: int = 30,
) -> Dict[str, Any]:
  """
  Send the uploaded image (and extracted metadata) to the remote ML API.

  Parameters
  ----------
  image_bytes:
      Raw image contents supplied by the client.
  metadata:
      EXIF-derived metadata that may enhance the ML model's accuracy.
  url:
      Fully-qualified endpoint for the ML service prediction REST API.
  api_key:
      Optional bearer token injected as ``Authorization`` header.
  timeout:
      Request timeout in seconds.
  """
  if not url:
    raise MLServiceError("ML service URL is not configured.")

  stub_mode = os.environ.get("ML_SERVICE_MODE", "").strip().lower()
  normalised_url = url.strip().lower()
  if stub_mode == "stub" or normalised_url in {"stub", "mock"} or normalised_url.startswith("stub://"):
    logger.info("Returning synthetic prediction from ML stub (mode=%s, url=%s).", stub_mode or "unset", url)
    return _build_stub_prediction()

  files = {
    "image": ("upload.jpg", image_bytes, "application/octet-stream"),
  }
  data = {
    "metadata": json.dumps(metadata or {}),
  }
  headers = {}
  if api_key:
    headers["Authorization"] = f"Bearer {api_key}"

  try:
    response = requests.post(url, files=files, data=data, headers=headers, timeout=timeout)
  except requests.RequestException as exc:  # pragma: no cover
    raise MLServiceError(f"ML service request failed: {exc}") from exc

  if not response.ok:
    raise MLServiceError(
      f"ML service responded with {response.status_code}: {response.text}"
    )

  try:
    return response.json()
  except ValueError as exc:  # pragma: no cover
    raise MLServiceError("ML service did not return JSON.") from exc


__all__ = ["call_ml_service", "MLServiceError"]
